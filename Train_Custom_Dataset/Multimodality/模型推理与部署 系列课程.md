## 基于BERT模型推理与部署系列

### 一、TensorRT + Triton

#### <font color=blue>1. TensorRT 容器化部署</font>

**1.1 [官网链接](https://developer.nvidia.com/tensorrt#what-is)**

**1.2 [github链接](https://github.com/NVIDIA/TensorRT)**

```python
# 拉取项目
git clone https://github.com/NVIDIA/TensorRT.git
```

**1.3 [镜像文件](https://docs.nvidia.com/deeplearning/tensorrt/container-release-notes/index.html)**

```python
# 拉取镜像
docker pull nvcr.io/nvidia/tensorrt:22.05-py3
```

**1.4 运行容器**

```py
# 运行容器
docker run --gpus all -itd --network=host -v $PWD/tensorrt_triton:/workspace/tensorrt_triton -p 9000:9000 --name tensorrt nvcr.io/nvidia/tensorrt:22.05-py3

# 进入容器 TensorRT
docker exec -it tensorrt /bin/bash

# 安装依赖
pip install tensorflow onnx keras2onnx tf2onnx scikit-learn scikit-image torch torchvision jupyter notebook
```

#### <font color=blue>2. Triton容器化部署</font>

**2.1 [官网链接](https://developer.nvidia.com/nvidia-triton-inference-server)**

**2.2 [github链接](https://github.com/triton-inference-server/server)**

```python
# 在本机路径下: /home/tgl/Desktop/Model_Inference_Deployment/tensorrt_triton 拉取项目
git clone https://github.com/triton-inference-server/server.git
# 进入examples
cd server/docs/examples
# 下载模型文件
./fetch_models.sh
```

**2.3 [镜像文件](https://docs.nvidia.com/deeplearning/triton-inference-server/release-notes/rel-22-12.html#rel-22-12)**

```PYTHON
# 拉取镜像
docker pull nvcr.io/nvidia/tritonserver:22.05-py3
```

**2.4 运行容器**

```python
# 在 /home/tgl/Desktop/Model_Inference_Deployment/tensorrt_triton/server/docs/examples
# 运行容器
docker run --gpus all -itd --net host --name triton-serve -p 8000:8000 -p 8001:8001 -p 8002:8002 -v $PWD/model_repository:/models nvcr.io/nvidia/tritonserver:22.05-py3 tritonserver --model-repository=/models

# 查看容器状态
docker ps

# 验证容器状态
重新打开一个命令行窗口，输入命令: curl -v localhost:8000/v2/health/ready
```

**2.5 基于trition client进行推理**

```python
# 拉取镜像
docker pull nvcr.io/nvidia/tritonserver:22.05-py3-sdk

# 运行容器
docker run -itd --net=host --name triton-client nvcr.io/nvidia/tritonserver:22.05-py3-sdk

# 进入容器
docker exec -it triton-client /bin/bash

# 推理测试
/workspace/install/bin/image_client -m densenet_onnx -c 3 -s INCEPTION /workspace/images/mug.jpg

# 输出结果
Request 0, batch size 1
Image '/workspace/images/mug.jpg':
    15.346230 (504) = COFFEE MUG
    13.224326 (968) = CUP
    10.422965 (505) = COFFEEPOT
```

#### <font color=blue>3. TensorRT + Triton 案例</font>

```python
# 进入容器
docker exec -it tensorrt /bin/bash

# 启动 jupyter notebook
jupyter notebook --allow-root
```

##### **案例1: Using Tensorflow 2 through ONNX**

```markdown
代码文件：

home/tgl/Desktop/Model_Inference_Deployment/tensorrt_triton/案例1 - Using Tensorflow through ONNX.ipynb'
```

##### **案例2: Using PyTorch through ONNX**

```markdown
代码文件：

home/tgl/Desktop/Model_Inference_Deployment/tensorrt_triton/案例2 - Using PyTorch through ONNX.ipynb'
```

#### <font color=blue>4. Triton 优化指南</font>

##### **4.1 Baseline performance**

```markdown
# 进入容器
docker exec -it triton-serve /bin/bash

# 执行命令 perf_analyzer 进行模型性能分析
perf_analyzer -m inception_graphdef --percentile=95 --concurrency-range 1:4
```

##### **4.2 推理优化配置**

```python
# 1. Dynamic Batcher 动态批处理

# ① 修改模型配置文件 config.pbtxt
路径：server/docs/examples/model_repository/inception_graphdef
添加一行代码：
-------------------------------
dynamic_batching { }
-------------------------------
修改完后，需要restart triton-serve容器

# ② 保存后，在容器triton-client下执行命令
perf_analyzer -m inception_graphdef --percentile=95 --concurrency-range 1:8

# ③ 如何计算request concurrency
2 * <maximum batch size> * <model instance count> = 并行请求量

perf_analyzer -m inception_graphdef --percentile=95 --concurrency-range 8
```

```python
# 2. Model Instances 模型实例
默认运行是1个模型实例，多个模型实例的作用：
(1) 提高模型推理性能；
(2) 提高GPU利用率；

# ① 修改模型配置文件 config.pbtxt
路径：server/docs/examples/model_repository/inception_graphdef
添加一行代码：
-------------------------------
instance_group [ { count: 2}]
-------------------------------
修改完后，需要restart triton-serve容器

# 保存后，在容器triton-client下执行命令
perf_analyzer -m inception_graphdef --percentile=95 --concurrency-range 1:4

```

```python
# 3. Framework-Specific Optimization 框架无关优化

 ## 3.1 ONNX with TensorRT Optimization (ORT-TRT)
  ① 基于densenet_onnx为baseline模型进行测试
  perf_analyzer -m densenet_onnx --percentile=95 --concurrency-range 1:4
  
  ② 修改配置文件server/docs/examples/model_repository/densenet_onnx/config.pbtxt，在最后添加以下代码段:
  --------------------------------------------------------------
  optimization { execution_accelerators {
  gpu_execution_accelerator : [{
    name: "tensorrt"
    parameters { key: "precision_mode" value: "FP16" }
    parameters { key: "max_workspace_size_bytes" value: "1073741824" }
  }]
}}
  --------------------------------------------------------------
  重启容器，docker restart triton-serve
  
  ③ 测试命令
  perf_analyzer -m densenet_onnx --percentile=95 --concurrency-range 1:4
  
  ## 3.2 TensorFlow with TensorRT Optimization (TF-TRT)
  ① 修改计算精度(compute precision)
  ----------------config.pbtxt----------------------
  optimization { execution_accelerators {
  gpu_execution_accelerator : [ {
    name: "tensorrt"
    parameters { key: "precision_mode" value: "FP16" }}]
}}
  --------------------------------------------------
  ② 测试
  perf_analyzer -m inception_graphdef --percentile=95 --concurrency-range 1:4
  
  ## 3.3 TensorFlow JIT Graph Optimizations
  ① 在运行模型图时，通过GlobalJitLevel设置，指定优化阶段
  -----------------config.pbtxt----------------
  optimization {
  graph { level: 1
  }}
  ---------------------------------------------
  
  ## 3.4 TensorFlow Automatic FP16 Optimization
  ① FP16优化
  -----------------config.pbtxt----------------
  optimization { execution_accelerators {
  gpu_execution_accelerator : [
    { name : "auto_mixed_precision" }
  ]
  }}
  ----------------------------------------------
  
  ② 测试
  perf_analyzer -m inception_graphdef --percentile=95 --concurrency-range 1:4
```

#### <font color=blue>5. Triton Model Analyzer 模型分析器</font>

```python
# 作用：
# (1) 最大化模型吞吐量；(2) 优化硬件使用；(3) 提高可读性；

# 1. 下载 add_sub model


# 2. Pull and Run the SDK Container
# ① 拉取 SDK 镜像
docker pull nvcr.io/nvidia/tritonserver:23.01-py3-sdk

# ② 运行SDK容器 
docker run -it --name triton_model_analyzer -v /var/run/docker.sock:/var/run/docker.sock -v $(pwd)/examples/quick-start:$(pwd)/examples/quick-start -v /Users/tgl/Desktop/Model_Inference_Deployment/tensorrt_triton/Triton_Model_Analyzer/model_outputs:/workspace/model_outputs --net=host nvcr.io/nvidia/tritonserver:23.01-py3-sdk

# ③ 进入容器
docker exec -it triton_model_analyzer /bin/bash

# 2. Profile the add_sub model
model-analyzer profile --model-repository /Users/tgl/Desktop/Model_Inference_Deployment/tensorrt_triton/Triton_Model_Analyzer/examples/quick-start --profile-models add_sub --triton-launch-mode=docker --output-model-repository-path /workspace/model_outputs/results --export-path /workspace/model_outputs/profile_results --run-config-search-max-concurrency 2 --run-config-search-max-model-batch-size 2 --run-config-search-max-instance-count 2 --override-output-model-repository

```

方案二

```python
# 1. 创建容器
docker pull nvcr.io/nvidia/pytorch:22.04-py3

docker run -itd -v $(pwd)/Triton_Model_Analyzer:/workspace --name pytorch nvcr.io/nvidia/pytorch:22.04-py3

# 2. 下载模型（将代码写入文件download_model.py, 容器外运行后自动下载）
from transformers import BertModel, BertTokenizer
import torch
model_name = "bert-large-uncased"
tokenizer = BertTokenizer.from_pretrained(model_name)
model = BertModel.from_pretrained(model_name, torchscript=True)
max_seq_len = 512
sample = "This is a sample input text"
tokenized = tokenizer(sample, return_tensors="pt", max_length=max_seq_len, padding="max_length", truncation=True)
inputs = (tokenized.data['input_ids'], tokenized.data['attention_mask'], tokenized.data['token_type_ids'])
traced_model = torch.jit.trace(model, inputs)
traced_model.save("model.pt")

# 3. 
```

### 二、Tensorflow + TF_serving

#### <font color=blue>1. 容器化部署</font>

**1.1 [官网链接](https://www.tensorflow.org/tfx/guide/serving)**

```python
# 1. 拉取镜像
# GPU
docker pull tensorflow/serving:latest-gpu

# CPU
docker pull tensorflow/serving:latest

# 2. 拉取项目
git clone https://github.com/tensorflow/serving

# 4. 运行容器
docker run --gpus all -p 8501:8501 \
    -v $PWD/serving/tensorflow_serving/servables/tensorflow/testdata/saved_model_half_plus_two_gpu:/models/half_plus_two \
    -e MODEL_NAME=half_plus_two \
    -t tensorflow/serving:latest-gpu &

docker run -p 8501:8501 \
    -v $PWD/serving/tensorflow_serving/servables/tensorflow/testdata/saved_model_half_plus_two_cpu:/models/half_plus_two \
    -e MODEL_NAME=half_plus_two \
    -t tensorflow/serving:latest &

# 5. 服务验证
curl -d '{"instances": [1.0, 2.0, 5.0]}' -X POST http://localhost:8501/v1/models/half_plus_two:predict
```

#### <font color=blue>2. TF_serving 案例</font>

##### **案例1: MNIST分类**

```python
# 1. 运行容器
docker run -it --gpus all --net host -p 8888:8888 -v $PWD/notebooks:/workspace/notebooks --name tf tensorflow/tensorflow:latest-gpu

# 2. 安装依赖
pip install jupyter notebook scikit-learn matplotlib grpcio==1.26.0

# 3. 启动notebook
jupyter notebook --allow-root
```

```python
# 4. 案例代码
代码路径: /home/tgl/Desktop/Model_Inference_Deployment/tf_serving/notebooks/tensorflow serving mnist classification.ipynb
```

##### **案例2: 线性回归**

###### **（1）单模型部署**

###### **（2）多模型部署**

```python
# 1. 运行环境与案例1一致，无需再次创建新的容器
docker exec -it tf /bin/bash

# 2. 案例代码
代码路径：/home/tgl/Desktop/Model_Inference_Deployment/tf_serving/notebooks/Regression Case.ipynb

# 3. 模型部署

# ① 运行容器

docker run --gpus all --net host --rm --name reg_demo -it -v $PWD/notebooks:/workspace/notebooks -p 9001:9001 --entrypoint /bin/bash tensorflow/serving:latest-gpu
            
# ② 启动服务

tensorflow_model_server --rest_api_port=9001 --model_name=regression_experiments --model_base_path=/workspace/reg_models

#  ③ 服务测试
curl -X GET http://localhost:9001/v1/models/regression_experiments
        
# ④ 测试
curl -X POST "http://localhost:9001/v1/models/regression_experiments:predict" -H "Content-Type: application/json" -d "{\"instances\":[[1.0], [2.0], [5.0]]}"

# 4. 部署多个模型

# ① 创建配置文件
在文件夹notebooks下，创建配置文件： conf_1.conf
--------------内容如下-----------------
model_config_list {
    config {
        name: "regression_experiments"
        base_path: "/workspace/notebooks/reg_models/"
        model_platform: "tensorflow"
        model_version_policy: {
         specific {
                versions: 1
            }   
        }
    }
}
--------------------------------------
# ② 启动服务
tensorflow_model_server --rest_api_port=9001 --allow_version_labels_for_unavailable_models --model_config_file=/workspace/notebooks/conf_1.conf

# ③ 服务验证
curl -X GET http://localhost:9001/v1/models/regression_experiments

# ④ 调用模型1的服务
curl -X POST "http://localhost:9001/v1/models/regression_experiments/versions/1:predict" -H "Content-Type: application/json" -d "{\"instances\":[[1.0], [2.0], [5.0]]}"

# ⑤ 调用模型2的服务
curl -X POST "http://localhost:9001/v1/models/regression_experiments/versions/2:predict" -H "Content-Type: application/json" -d "{\"instances\":[[1.0], [2.0], [5.0]]}"

# 5. 自定义模型版本标签

# ① 在文件夹notebooks下，创建配置文件： conf_2.conf
----------------内容如下---------------
model_config_list {
    config {
        name: "regression_experiments"
        base_path: "/workspace/notebooks/reg_models/"
        model_platform: "tensorflow"
        model_version_policy {
            specific {
                versions: 1
                versions： 2
            }
        }
        version_labels {
            key: "production"
            value: 1
        }
        version_labels {
            key: "test"
            value: 2
        }
    }
}
-----------------------------------------
# ② 启动服务
tensorflow_model_server --rest_api_port=9001 --allow_version_labels_for_unavailable_models --model_config_file=/workspace/notebooks/conf_2.conf

# ③ 调用模型1的服务【注意这里用的是 labels/production】
curl -X POST "http://localhost:9001/v1/models/regression_experiments/labels/production:predict" \
-H "Content-Type: application/json" \
-d "{\"instances\":[[1.0], [2.0], [5.0]]}"

# ④ 调用模型2的服务【注意这里用的是 labels/test】
curl -X POST "http://localhost:9001/v1/models/regression_experiments/labels/test:predict" \
-H "Content-Type: application/json" \
-d "{\"instances\":[[1.0], [2.0], [5.0]]}"


# 6. 自动化加载配置文件【例如：间隔一段时间加载一次】

# ① 重新启动服务【注：每10秒读取一次】
tensorflow_model_server 
  --rest_api_port=9001 
  --allow_version_labels_for_unavailable_models 
  --model_config_file=/myProject/cfg2.conf 
  --model_config_file_poll_wait_seconds=10

```

### 三、PyTorch + Torch_serve

#### <font color=blue>1. 容器化部署</font>

```python
【 我的本机当前路径：/Desktop/Model_Deployment/Torch_Server 】

# 0. 拉取项目
git clone https://github.com/pytorch/serve.git

# 1. 运行容器
docker run --net host --gpus all --rm -it -p 8080:8080 -p 8081:8081 -p 8082:8082 -p 7070:7070 -p 7071:7071 --name mar -v $(pwd)/model-store:/home/model-server/model-store -v $(pwd)/serve/examples:/home/model-server/examples pytorch/torchserve:latest-gpu


【注意：① TorchServe使用默认端口8080/8081/8082 for REST based inference,management&metrics APIs，② 7070/7071 for gRPC APIs】

# 2. 进入容器
docker exec -it mar/bin/bash

# 3. 下载模型文件
curl -o /home/model-server/examples/image_classifier/densenet161-8d451a50.pth https://download.pytorch.org/models/densenet161-8d451a50.pth

【注意：需要提前创建好完整的路径：/home/model-server/examples/image_classifier/，不然下载会报错找不到路径】

# 4. 运行torch-model-archiver命令
torch-model-archiver --model-name densenet161 --version 1.0 --model-file /home/model-server/examples/image_classifier/densenet_161/model.py --serialized-file /home/model-server/examples/image_classifier/densenet161-8d451a50.pth --export-path /home/model-server/model-store --extra-files /home/model-server/examples/image_classifier/index_to_name.json --handler image_classifier

【注意：--handler 的候选项有：image_classifier，object_detector，text_classifier，image_segmenter】
```

[命令 torch-model-archiver 的候选参数说明  链接](https://github.com/pytorch/serve/blob/master/model-archiver/README.md#arguments)

#### <font color=blue>2. Torch_serve 案例</font>

##### **案例1: Sequence Classification**

```python
# 第1步：下载预训练模型
cd serve/examples/Huggingface_Transformers  
# serve是通过git clone https://github.com/pytorch/serve.git下载下来的项目

python Download_Transformer_models.py # 下载模型

# 第2步：创建model archive eager mode
# ① 进入容器
docker exec -it mar /bin/bash
cd /home/model-server/examples/Huggingface_Transformers

# ② 运行命令
torch-model-archiver --model-name BERTSeqClassification --version 1.0 --serialized-file Transformer_model/pytorch_model.bin --handler ./Transformer_handler_generalized.py --extra-files "Transformer_model/config.json,./setup_config.json,./Seq_classification_artifacts/index_to_name.json"
#【 输出 BERTSeqClassification.mar 】

# 第3步：注册模型
# 容器中的路径：/home/model-server/examples/Huggingface_Transformers
mkdir model_store
mv BERTSeqClassification.mar model_store/
torchserve --start --model-store model_store --models my_tc=BERTSeqClassification.mar --ncs

# 第4步：运行 inference
curl -X POST http://127.0.0.1:8080/predictions/my_tc -T Seq_classification_artifacts/sample_text_captum_input.txt

curl -X POST http://127.0.0.1:8080/explanations/my_tc -T Seq_classification_artifacts/sample_text_captum_input.txt
```

##### **案例2: Question Answering**

```python
# 项目路径：serve/examples/Huggingface_Transformers

# 第1步：删除上一个案例的 Transformer_model 文件夹
rm -rf Transformer_model/

# 第2步：修改 setup_config.json
{
 "model_name":"distilbert-base-cased-distilled-squad",
 "mode":"question_answering",
 "do_lower_case":true,
 "num_labels":"0",
 "save_mode":"pretrained",
 "max_length":"128",
 "captum_explanation":true,
 "FasterTransformer":false,
 "embedding_name": "distilbert"
}

# 第3步：下载预训练模型
python Download_Transformer_models.py

# 第4步：创建model archive eager mode
# 注意：这一步的操作在容器中进行
torch-model-archiver --model-name BERTQA --version 1.0 --serialized-file Transformer_model/pytorch_model.bin --handler ./Transformer_handler_generalized.py --extra-files "Transformer_model/config.json,./setup_config.json"

# 第5步：注册模型
mkdir model_store
mv BERTQA.mar model_store
torchserve --start --model-store model_store --models my_tc=BERTQA.mar --ncs

# 第6步：运行 inference
curl -X POST http://127.0.0.1:8080/predictions/my_tc -T QA_artifacts/sample_text_captum_input.txt

curl -X POST http://127.0.0.1:8080/explanations/my_tc -T QA_artifacts/sample_text_captum_input.txt
```

##### **案例3: Batch Inference**

```python
# 工作路径：容器内 /home/model-server/examples/Huggingface_Transformers
# 第1步：创建一个文件，config.properties,写入以下内容:
models={\
  "BERTQA": {\
    "2.0": {\
      "defaultVersion": true,\
      "marName": "BERTQA.mar",\
      "minWorkers": 1,\
      "maxWorkers": 1,\
      "batchSize": 4,\
      "maxBatchDelay": 50,\
      "responseTimeout": 120\
    }\
  }\
}

# 第2步：运行服务
torchserve --start --model-store model_store --ts-config config.properties --models BERTQA= BERTQA.mar

# 第3步：batch inference
curl -X POST http://127.0.0.1:8080/predictions/BERTQA  -T ./QA_artifacts/sample_text1.txt & curl -X POST http://127.0.0.1:8080/predictions/BERTQA  -T ./QA_artifacts/sample_text2.txt & curl -X POST http://127.0.0.1:8080/predictions/BERTQA -T ./QA_artifacts/sample_text3.txt &
```

**<font color=blue>进一步了解 Batch Inference with TorchServe</font> [链接](https://github.com/pytorch/serve/blob/master/docs/batch_inference_with_ts.md)**

##### **案例4：使用默认的text_classifier handler进行文本分类**

[官网案例链接](https://github.com/pytorch/serve/tree/master/examples/text_classification)

【注意：该案例在容器mar中运行，通过 docker ps查看】

【容器内路径： /home/model-server/examples/text_classification】

```python
# 第1步：模型训练，执行脚本
python run_script.py

# 第2步：创建 torch model archive
torch-model-archiver --model-name my_text_classifier --version 1.0 --model-file model.py --serialized-file model.pt  --handler text_classifier --extra-files "index_to_name.json,source_vocab.pt"

# 第3步：启动服务
mkdir model_store

mv my_text_classifier.mar model_store/

torchserve --start --model-store model_store --models my_tc=my_text_classifier.mar
curl http://127.0.0.1:8080/predictions/my_tc -T examples/text_classification/sample_text.txt

# 第4步：服务测试
curl -X POST http://127.0.0.1:8080/explanations/my_tc -T examples/text_classification/sample_text.txt
```

##### **案例5: mnist 分类**

```python
# 第1步：运行容器
docker run --net host --gpus all --rm -it -p 8080:8080 -p 8081:8081 -p 8082:8082 -p 7070:7070 -p 7071:7071 --name mnist_case -v $(pwd)/model-store:/home/model-server/model-store -v $(pwd)/serve/examples:/home/model-server/examples pytorch/torchserve:latest-gpu

# 第2步：进入容器
docker exec -it mnist_case /bin/bash

# 第3步：生成mar文件
 torch-model-archiver --model-name mnist --version 1.0 --model-file examples/mnist.py --serialized-file examples/mnist_cnn.pt --handler examples/mnist_handler.py
    
# 第4步：启动服务
export ENABLE_TORCH_PROFILLER=true

torchserve --start --model-store model-store --models mnist=mnist.mar --ts-config examples/image_classifier/mnist/config.properties

# 第5步：测试服务
curl http://127.0.0.1:8080/predictions/mnist -T examples/image_classifier/mnist/test_data/9.png
```

### 推理优化配置

### Triton model analyzer 分析器

### 附录: 案例参考链接

```markd
# TensorRT + Triton


https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/getting_started/quickstart.html
```
